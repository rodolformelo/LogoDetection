{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "1. Split_and_tfrecords",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sbaAioTx5TGv"
   },
   "source": [
    "## Deep Learning for Computer Vision\n",
    "\n",
    "### Logo Detection\n",
    "### Bocconi University\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Eof679xfBs2P"
   },
   "source": [
    "Please, use a GPU session."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ZTgY82dqPycc"
   },
   "source": [
    "\"\"\"\n",
    "  IMPORTING LIBRARIES\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from os import listdir\n",
    "from os.path import join, isfile"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YItt22Wm6zYc"
   },
   "source": [
    "### 1. Split the data\n",
    "We start by importing the dataset from dropbox and splitting it into train, validation and test set. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K25dpe4N3yTF",
    "outputId": "a55d7e27-a86e-4281-a6fe-81591fd43980"
   },
   "source": [
    "\"\"\"\n",
    "  IMPORTING TAR DATASET FROM DROPBOX\n",
    "\"\"\"\n",
    "\n",
    "!wget https://www.dropbox.com/s/nkoxs4boe8m48xf/DLCV_logo_project.tar.gz"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "--2021-12-02 16:18:33--  https://www.dropbox.com/s/nkoxs4boe8m48xf/DLCV_logo_project.tar.gz\n",
      "Resolving www.dropbox.com (www.dropbox.com)... 162.125.3.18, 2620:100:6018:18::a27d:312\n",
      "Connecting to www.dropbox.com (www.dropbox.com)|162.125.3.18|:443... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: /s/raw/nkoxs4boe8m48xf/DLCV_logo_project.tar.gz [following]\n",
      "--2021-12-02 16:18:33--  https://www.dropbox.com/s/raw/nkoxs4boe8m48xf/DLCV_logo_project.tar.gz\n",
      "Reusing existing connection to www.dropbox.com:443.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://ucd19277bf59dba98a7d654c8c2d.dl.dropboxusercontent.com/cd/0/inline/BbHKlQB-F-355BZgu_vl0MFvB7ef92wuM9xcvp6KcuMyI6xyHt7iUsxmm-cVSnxcQFd3fq2sbyC3z_WWut1wW8bmlaui65JWaTCkhOCf_dJ-DcHk1tG9juLMbjGn3FQCYEgYo-O0XHWPR47eGWf2AWdj/file# [following]\n",
      "--2021-12-02 16:18:33--  https://ucd19277bf59dba98a7d654c8c2d.dl.dropboxusercontent.com/cd/0/inline/BbHKlQB-F-355BZgu_vl0MFvB7ef92wuM9xcvp6KcuMyI6xyHt7iUsxmm-cVSnxcQFd3fq2sbyC3z_WWut1wW8bmlaui65JWaTCkhOCf_dJ-DcHk1tG9juLMbjGn3FQCYEgYo-O0XHWPR47eGWf2AWdj/file\n",
      "Resolving ucd19277bf59dba98a7d654c8c2d.dl.dropboxusercontent.com (ucd19277bf59dba98a7d654c8c2d.dl.dropboxusercontent.com)... 162.125.3.15, 2620:100:6018:15::a27d:30f\n",
      "Connecting to ucd19277bf59dba98a7d654c8c2d.dl.dropboxusercontent.com (ucd19277bf59dba98a7d654c8c2d.dl.dropboxusercontent.com)|162.125.3.15|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: /cd/0/inline2/BbHrdrKKJgURrqEGfld_8ScatacrQZlFRo4dryOQ02EFNhs5MRl1Ez3pk61X7NcUJ6X3SB2BsuIvn5EuppijFrw84nrR-WAb3VNwtc4JEJjfl9_Z-iOmTVP7MjM6nVpjE93bihF33UWzesa1rMufmtHFDcHO8VbLI6HA3eXlmHs0KdBySQLfb-lSxClPA--NrdulengSljVxshiYxm1GomKaG72SguKSmohDSTahRvUP8moMhS_nT0Os_ITtKHUKnkluzDjqNK2pnxR9u7PJu0EOUl-g_MBzlSOn5tcQkl-19sEhJ9fPnsQAkKBxxBVZBrgCJ7zeLkWjiqqnrwH3LzEBOC_nnDv2PasVGY1Dmp1SjSo_1pQsgXhlJbHY0dlQkfQ/file [following]\n",
      "--2021-12-02 16:18:34--  https://ucd19277bf59dba98a7d654c8c2d.dl.dropboxusercontent.com/cd/0/inline2/BbHrdrKKJgURrqEGfld_8ScatacrQZlFRo4dryOQ02EFNhs5MRl1Ez3pk61X7NcUJ6X3SB2BsuIvn5EuppijFrw84nrR-WAb3VNwtc4JEJjfl9_Z-iOmTVP7MjM6nVpjE93bihF33UWzesa1rMufmtHFDcHO8VbLI6HA3eXlmHs0KdBySQLfb-lSxClPA--NrdulengSljVxshiYxm1GomKaG72SguKSmohDSTahRvUP8moMhS_nT0Os_ITtKHUKnkluzDjqNK2pnxR9u7PJu0EOUl-g_MBzlSOn5tcQkl-19sEhJ9fPnsQAkKBxxBVZBrgCJ7zeLkWjiqqnrwH3LzEBOC_nnDv2PasVGY1Dmp1SjSo_1pQsgXhlJbHY0dlQkfQ/file\n",
      "Reusing existing connection to ucd19277bf59dba98a7d654c8c2d.dl.dropboxusercontent.com:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 7343185920 (6.8G) [application/octet-stream]\n",
      "Saving to: ‘DLCV_logo_project.tar.gz’\n",
      "\n",
      "DLCV_logo_project.t 100%[===================>]   6.84G  32.7MB/s    in 2m 17s  \n",
      "\n",
      "2021-12-02 16:20:51 (51.2 MB/s) - ‘DLCV_logo_project.tar.gz’ saved [7343185920/7343185920]\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "TSfTY40V_eNy"
   },
   "source": [
    "!tar -xf DLCV_logo_project.tar.gz # untar dataset"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "CmqQn47SPGLQ"
   },
   "source": [
    "\"\"\"\n",
    "  CREATING LOCAL PATHS\n",
    "\"\"\"\n",
    "\n",
    "path = '/content'\n",
    "\n",
    "# Defining directories' path\n",
    "data_dir = join(path, 'data')\n",
    "train_dir = join(data_dir, 'train')\n",
    "valid_dir = join(data_dir, 'valid')\n",
    "test_dir = join(data_dir, 'test')\n",
    "\n",
    "# Creating directories\n",
    "os.mkdir(data_dir)\n",
    "os.mkdir(train_dir)\n",
    "os.mkdir(valid_dir)\n",
    "os.mkdir(test_dir)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "05Az0Q91QIAr"
   },
   "source": [
    "\"\"\"\n",
    "  FUNCTION THAT RECEIVES THE ORIGINAL PATH OF THE PHOTO FOLDER, \n",
    "  THE TARGET PATH FOR THE COPY FOLDER,\n",
    "  DATAFRAME THAT THE FIRST COLUMN IS THE NAME OF THE PHOTO FILE IN ORIGINAL PATH\n",
    "\"\"\"\n",
    "def change_directory(original,target,df_photo_filename):\n",
    "    for index, rows in (df_photo_filename.iterrows()):\n",
    "        new_original = join(original, rows.values[0])\n",
    "        new_target = join(target, rows.values[0])\n",
    "        shutil.copyfile(new_original, new_target)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "  SPLITTING FILES USING ALL ORIGINAL PHOTOS AND PANDAS DATAFRAME SAMPLE FOR:\n",
    "    - 80% TRAIN SET\n",
    "    - 10% VALID SET\n",
    "    - 10% TEST SET\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Original data path\n",
    "original_train = r'/content/DLCV_logo_project/train'\n",
    "\n",
    "# Check the name of all images in original train to avoid errors\n",
    "onlyfiles = [f for f in listdir(original_train) if isfile(join(original_train, f))]\n",
    "\n",
    "# Annotation to dataframe\n",
    "df = pd.read_csv('/content/DLCV_logo_project/annot_train.csv')\n",
    "df = df[df['photo_filename'].isin(onlyfiles)] # filter for onlyfiles\n",
    "\n",
    "# 14 Logos\n",
    "logos = [\"Adidas\",\"Apple Inc.\",\"Chanel\",\"Coca-Cola\",\"Hard Rock Cafe\",\"Mercedes-Benz\",\n",
    "         \"NFL\",\"Nike\",\"Pepsi\",\"Puma\",\"Starbucks\",\"The North Face\",\"Toyota\",\"Under Armour\"]\n",
    "df = df[df['class'].isin(logos)]\n",
    "\n",
    "# Split train dataframe\n",
    "df_train = df.sample(frac=0.8, random_state=0).reset_index(drop=True)\n",
    "df_test_valid = df[~df['photo_filename'].isin(df_train['photo_filename'])].reset_index(drop=True)\n",
    "\n",
    "# Split valid and test dataframes\n",
    "df_valid = df_test_valid.sample(frac=0.5, random_state=0).reset_index(drop=True)\n",
    "df_test = df_test_valid[~df_test_valid['photo_filename'].isin(df_valid['photo_filename'])].reset_index(drop=True)\n",
    "\n",
    "# Final split\n",
    "df_photo_filename_train = df_train[['photo_filename']]\n",
    "df_photo_filename_valid = df_valid[['photo_filename']]\n",
    "df_photo_filename_test = df_test[['photo_filename']]\n",
    "\n",
    "# Copy splitted files to new origin\n",
    "change_directory(original_train, train_dir, df_photo_filename_train)\n",
    "change_directory(original_train, valid_dir, df_photo_filename_valid)\n",
    "change_directory(original_train, test_dir, df_photo_filename_test)\n",
    "\n",
    "# Save data frames\n",
    "df_train.to_csv(join(data_dir, 'train.csv'), index=False)\n",
    "df_valid.to_csv(join(data_dir, 'valid.csv'), index=False)\n",
    "df_test.to_csv(join(data_dir, 'test.csv'), index=False)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "OAZOYDEICKXh",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 488
    },
    "outputId": "42c1ca72-8dcc-49d7-dda6-6487f2ce903c"
   },
   "source": [
    "\"\"\"\n",
    "  STATISTICS OF THE TRAIN/VALID/TEST SETS\n",
    "\"\"\"\n",
    "\n",
    "stats_split = dict()\n",
    "\n",
    "for classe in df['class'].unique():\n",
    "  total = len(df[df['class']==classe])\n",
    "  stats = [total,\n",
    "           len(df_train[df_train['class']==classe])*100/total,\n",
    "            len(df_valid[df_valid['class']==classe])*100/total,\n",
    "            len(df_test[df_test['class']==classe])*100/total]\n",
    "  stats_split[classe] = stats\n",
    "  \n",
    "stats_split = pd.DataFrame(stats_split, \n",
    "                           columns = stats_split.keys(), \n",
    "                           index = ['Original','Train %', 'Valid %', 'Test %']).T\n",
    "stats_split = stats_split.sort_values(by=['Original'],ascending=False)\n",
    "\n",
    "stats_split"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Original</th>\n",
       "      <th>Train %</th>\n",
       "      <th>Valid %</th>\n",
       "      <th>Test %</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Nike</th>\n",
       "      <td>9566.0</td>\n",
       "      <td>79.960276</td>\n",
       "      <td>9.983274</td>\n",
       "      <td>10.056450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Adidas</th>\n",
       "      <td>8119.0</td>\n",
       "      <td>79.874369</td>\n",
       "      <td>9.902697</td>\n",
       "      <td>10.222934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Starbucks</th>\n",
       "      <td>3660.0</td>\n",
       "      <td>79.699454</td>\n",
       "      <td>10.273224</td>\n",
       "      <td>10.027322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mercedes-Benz</th>\n",
       "      <td>2089.0</td>\n",
       "      <td>80.947822</td>\n",
       "      <td>8.903782</td>\n",
       "      <td>10.148396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NFL</th>\n",
       "      <td>2079.0</td>\n",
       "      <td>80.952381</td>\n",
       "      <td>10.582011</td>\n",
       "      <td>8.465608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Apple Inc.</th>\n",
       "      <td>1859.0</td>\n",
       "      <td>80.527165</td>\n",
       "      <td>9.252286</td>\n",
       "      <td>10.220549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Under Armour</th>\n",
       "      <td>1467.0</td>\n",
       "      <td>80.027267</td>\n",
       "      <td>10.633947</td>\n",
       "      <td>9.338787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Coca-Cola</th>\n",
       "      <td>1131.0</td>\n",
       "      <td>80.017683</td>\n",
       "      <td>9.549072</td>\n",
       "      <td>10.433245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Puma</th>\n",
       "      <td>964.0</td>\n",
       "      <td>78.734440</td>\n",
       "      <td>10.269710</td>\n",
       "      <td>10.995851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hard Rock Cafe</th>\n",
       "      <td>954.0</td>\n",
       "      <td>79.559748</td>\n",
       "      <td>10.901468</td>\n",
       "      <td>9.538784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>The North Face</th>\n",
       "      <td>858.0</td>\n",
       "      <td>80.419580</td>\n",
       "      <td>10.256410</td>\n",
       "      <td>9.324009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Chanel</th>\n",
       "      <td>774.0</td>\n",
       "      <td>78.940568</td>\n",
       "      <td>10.594315</td>\n",
       "      <td>10.465116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Toyota</th>\n",
       "      <td>754.0</td>\n",
       "      <td>79.575597</td>\n",
       "      <td>10.477454</td>\n",
       "      <td>9.946950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pepsi</th>\n",
       "      <td>623.0</td>\n",
       "      <td>79.935795</td>\n",
       "      <td>9.791332</td>\n",
       "      <td>10.272873</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Original    Train %    Valid %     Test %\n",
       "Nike              9566.0  79.960276   9.983274  10.056450\n",
       "Adidas            8119.0  79.874369   9.902697  10.222934\n",
       "Starbucks         3660.0  79.699454  10.273224  10.027322\n",
       "Mercedes-Benz     2089.0  80.947822   8.903782  10.148396\n",
       "NFL               2079.0  80.952381  10.582011   8.465608\n",
       "Apple Inc.        1859.0  80.527165   9.252286  10.220549\n",
       "Under Armour      1467.0  80.027267  10.633947   9.338787\n",
       "Coca-Cola         1131.0  80.017683   9.549072  10.433245\n",
       "Puma               964.0  78.734440  10.269710  10.995851\n",
       "Hard Rock Cafe     954.0  79.559748  10.901468   9.538784\n",
       "The North Face     858.0  80.419580  10.256410   9.324009\n",
       "Chanel             774.0  78.940568  10.594315  10.465116\n",
       "Toyota             754.0  79.575597  10.477454   9.946950\n",
       "Pepsi              623.0  79.935795   9.791332  10.272873"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "8AGtgsVsCrUn"
   },
   "source": [
    "\"\"\"\n",
    "  REMOVE TAR.GZ FILE TO SAVE LOCAL SPACE\n",
    "\"\"\"\n",
    "!rm /content/DLCV_logo_project.tar.gz"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ESatgcVmkruM"
   },
   "source": [
    "###2. Create tf records"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "qhQPsO7ykjwM",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "c75010c8-f5df-430f-bcf2-fa73342add93"
   },
   "source": [
    "# Clone the tensorflow models repository\n",
    "!git clone --depth 1 https://github.com/tensorflow/models"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Cloning into 'models'...\n",
      "remote: Enumerating objects: 3137, done.\u001B[K\n",
      "remote: Counting objects: 100% (3137/3137), done.\u001B[K\n",
      "remote: Compressing objects: 100% (2652/2652), done.\u001B[K\n",
      "remote: Total 3137 (delta 809), reused 1343 (delta 441), pack-reused 0\u001B[K\n",
      "Receiving objects: 100% (3137/3137), 33.35 MiB | 14.36 MiB/s, done.\n",
      "Resolving deltas: 100% (809/809), done.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C5KTMzzQycU4",
    "outputId": "d5dab4d1-63d3-424a-b85a-5504322d667b"
   },
   "source": [
    "%%shell\n",
    "python -m pip install --upgrade pip\n",
    "sudo apt install -y protobuf-compiler\n",
    "cd models/research/\n",
    "protoc object_detection/protos/*.proto --python_out=.\n",
    "cp object_detection/packages/tf2/setup.py .\n",
    "python -m pip install ."
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: pip in /usr/local/lib/python3.7/dist-packages (21.1.3)\n",
      "Collecting pip\n",
      "  Downloading pip-21.3.1-py3-none-any.whl (1.7 MB)\n",
      "\u001B[K     |████████████████████████████████| 1.7 MB 5.4 MB/s \n",
      "\u001B[?25hInstalling collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 21.1.3\n",
      "    Uninstalling pip-21.1.3:\n",
      "      Successfully uninstalled pip-21.1.3\n",
      "Successfully installed pip-21.3.1\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "protobuf-compiler is already the newest version (3.0.0-9.1ubuntu1).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 37 not upgraded.\n",
      "Processing /content/models/research\n",
      "  Preparing metadata (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "Collecting avro-python3\n",
      "  Downloading avro-python3-1.10.2.tar.gz (38 kB)\n",
      "  Preparing metadata (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "Collecting apache-beam\n",
      "  Downloading apache_beam-2.34.0-cp37-cp37m-manylinux2010_x86_64.whl (9.8 MB)\n",
      "     |████████████████████████████████| 9.8 MB 8.9 MB/s            \n",
      "\u001B[?25hRequirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (7.1.2)\n",
      "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (4.2.6)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (3.2.2)\n",
      "Requirement already satisfied: Cython in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (0.29.24)\n",
      "Requirement already satisfied: contextlib2 in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (0.5.5)\n",
      "Collecting tf-slim\n",
      "  Downloading tf_slim-1.1.0-py2.py3-none-any.whl (352 kB)\n",
      "     |████████████████████████████████| 352 kB 41.0 MB/s            \n",
      "\u001B[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (1.15.0)\n",
      "Requirement already satisfied: pycocotools in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (2.0.3)\n",
      "Collecting lvis\n",
      "  Downloading lvis-0.5.3-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (1.4.1)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from object-detection==0.1) (1.1.5)\n",
      "Collecting tf-models-official>=2.5.1\n",
      "  Downloading tf_models_official-2.7.0-py2.py3-none-any.whl (1.8 MB)\n",
      "     |████████████████████████████████| 1.8 MB 31.3 MB/s            \n",
      "\u001B[?25hCollecting tensorflow_io\n",
      "  Downloading tensorflow_io-0.22.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (22.7 MB)\n",
      "     |████████████████████████████████| 22.7 MB 1.8 MB/s             \n",
      "\u001B[?25hCollecting keras==2.6.0\n",
      "  Downloading keras-2.6.0-py2.py3-none-any.whl (1.3 MB)\n",
      "     |████████████████████████████████| 1.3 MB 34.5 MB/s            \n",
      "\u001B[?25hCollecting seqeval\n",
      "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
      "     |████████████████████████████████| 43 kB 1.8 MB/s             \n",
      "\u001B[?25h  Preparing metadata (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "Requirement already satisfied: google-api-python-client>=1.6.7 in /usr/local/lib/python3.7/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (1.12.8)\n",
      "Requirement already satisfied: tensorflow>=2.7.0 in /usr/local/lib/python3.7/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (2.7.0)\n",
      "Collecting sacrebleu\n",
      "  Downloading sacrebleu-2.0.0-py3-none-any.whl (90 kB)\n",
      "     |████████████████████████████████| 90 kB 8.4 MB/s             \n",
      "\u001B[?25hCollecting pyyaml>=5.1\n",
      "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
      "     |████████████████████████████████| 596 kB 37.2 MB/s            \n",
      "\u001B[?25hRequirement already satisfied: psutil>=5.4.3 in /usr/local/lib/python3.7/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (5.4.8)\n",
      "Requirement already satisfied: oauth2client in /usr/local/lib/python3.7/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (4.1.3)\n",
      "Collecting tensorflow-addons\n",
      "  Downloading tensorflow_addons-0.15.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
      "     |████████████████████████████████| 1.1 MB 35.4 MB/s            \n",
      "\u001B[?25hRequirement already satisfied: kaggle>=1.3.9 in /usr/local/lib/python3.7/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (1.5.12)\n",
      "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (1.19.5)\n",
      "Requirement already satisfied: tensorflow-hub>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (0.12.0)\n",
      "Requirement already satisfied: gin-config in /usr/local/lib/python3.7/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (0.5.0)\n",
      "Collecting tensorflow-text>=2.7.0\n",
      "  Downloading tensorflow_text-2.7.3-cp37-cp37m-manylinux2010_x86_64.whl (4.9 MB)\n",
      "     |████████████████████████████████| 4.9 MB 38.0 MB/s            \n",
      "\u001B[?25hCollecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "     |████████████████████████████████| 1.2 MB 21.7 MB/s            \n",
      "\u001B[?25hCollecting tensorflow-model-optimization>=0.4.1\n",
      "  Downloading tensorflow_model_optimization-0.7.0-py2.py3-none-any.whl (213 kB)\n",
      "     |████████████████████████████████| 213 kB 46.9 MB/s            \n",
      "\u001B[?25hCollecting opencv-python-headless\n",
      "  Downloading opencv_python_headless-4.5.4.60-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (47.6 MB)\n",
      "     |████████████████████████████████| 47.6 MB 65 kB/s              \n",
      "\u001B[?25hCollecting py-cpuinfo>=3.3.0\n",
      "  Downloading py-cpuinfo-8.0.0.tar.gz (99 kB)\n",
      "     |████████████████████████████████| 99 kB 9.2 MB/s             \n",
      "\u001B[?25h  Preparing metadata (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.7/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (4.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->object-detection==0.1) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->object-detection==0.1) (2018.9)\n",
      "Requirement already satisfied: absl-py>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from tf-slim->object-detection==0.1) (0.12.0)\n",
      "Requirement already satisfied: protobuf<4,>=3.12.2 in /usr/local/lib/python3.7/dist-packages (from apache-beam->object-detection==0.1) (3.17.3)\n",
      "Requirement already satisfied: httplib2<0.20.0,>=0.8 in /usr/local/lib/python3.7/dist-packages (from apache-beam->object-detection==0.1) (0.17.4)\n",
      "Requirement already satisfied: pymongo<4.0.0,>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam->object-detection==0.1) (3.12.1)\n",
      "Collecting dill<0.3.2,>=0.3.1.1\n",
      "  Downloading dill-0.3.1.1.tar.gz (151 kB)\n",
      "     |████████████████████████████████| 151 kB 45.0 MB/s            \n",
      "\u001B[?25h  Preparing metadata (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "Requirement already satisfied: typing-extensions<4,>=3.7.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam->object-detection==0.1) (3.10.0.2)\n",
      "Collecting avro-python3\n",
      "  Downloading avro-python3-1.9.2.1.tar.gz (37 kB)\n",
      "  Preparing metadata (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "Requirement already satisfied: grpcio<2,>=1.29.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam->object-detection==0.1) (1.42.0)\n",
      "Collecting orjson<4.0\n",
      "  Downloading orjson-3.6.4-cp37-cp37m-manylinux_2_24_x86_64.whl (249 kB)\n",
      "     |████████████████████████████████| 249 kB 48.6 MB/s            \n",
      "\u001B[?25hRequirement already satisfied: pyarrow<6.0.0,>=0.15.1 in /usr/local/lib/python3.7/dist-packages (from apache-beam->object-detection==0.1) (3.0.0)\n",
      "Requirement already satisfied: crcmod<2.0,>=1.7 in /usr/local/lib/python3.7/dist-packages (from apache-beam->object-detection==0.1) (1.7)\n",
      "Collecting hdfs<3.0.0,>=2.1.0\n",
      "  Downloading hdfs-2.6.0-py3-none-any.whl (33 kB)\n",
      "Collecting requests<3.0.0,>=2.24.0\n",
      "  Downloading requests-2.26.0-py2.py3-none-any.whl (62 kB)\n",
      "     |████████████████████████████████| 62 kB 599 kB/s             \n",
      "\u001B[?25hCollecting fastavro<2,>=0.21.4\n",
      "  Downloading fastavro-1.4.7-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n",
      "     |████████████████████████████████| 2.3 MB 39.4 MB/s            \n",
      "\u001B[?25hRequirement already satisfied: pydot<2,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from apache-beam->object-detection==0.1) (1.3.0)\n",
      "Collecting future<1.0.0,>=0.18.2\n",
      "  Downloading future-0.18.2.tar.gz (829 kB)\n",
      "     |████████████████████████████████| 829 kB 37.5 MB/s            \n",
      "\u001B[?25h  Preparing metadata (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "Requirement already satisfied: kiwisolver>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from lvis->object-detection==0.1) (1.3.2)\n",
      "Requirement already satisfied: pyparsing>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from lvis->object-detection==0.1) (3.0.6)\n",
      "Requirement already satisfied: opencv-python>=4.1.0.25 in /usr/local/lib/python3.7/dist-packages (from lvis->object-detection==0.1) (4.1.2.30)\n",
      "Requirement already satisfied: cycler>=0.10.0 in /usr/local/lib/python3.7/dist-packages (from lvis->object-detection==0.1) (0.11.0)\n",
      "Requirement already satisfied: setuptools>=18.0 in /usr/local/lib/python3.7/dist-packages (from pycocotools->object-detection==0.1) (57.4.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem==0.22.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_io->object-detection==0.1) (0.22.0)\n",
      "Requirement already satisfied: google-auth>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (1.35.0)\n",
      "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (3.0.1)\n",
      "Requirement already satisfied: google-api-core<2dev,>=1.21.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (1.26.3)\n",
      "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (0.0.4)\n",
      "Requirement already satisfied: docopt in /usr/local/lib/python3.7/dist-packages (from hdfs<3.0.0,>=2.1.0->apache-beam->object-detection==0.1) (0.6.2)\n",
      "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (5.0.2)\n",
      "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (1.24.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (4.62.3)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (2021.10.8)\n",
      "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.7/dist-packages (from oauth2client->tf-models-official>=2.5.1->object-detection==0.1) (0.4.8)\n",
      "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from oauth2client->tf-models-official>=2.5.1->object-detection==0.1) (4.8)\n",
      "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.7/dist-packages (from oauth2client->tf-models-official>=2.5.1->object-detection==0.1) (0.2.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam->object-detection==0.1) (2.0.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam->object-detection==0.1) (2.10)\n",
      "Requirement already satisfied: tensorboard~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.7.0->tf-models-official>=2.5.1->object-detection==0.1) (2.7.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.7.0->tf-models-official>=2.5.1->object-detection==0.1) (3.3.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.7.0->tf-models-official>=2.5.1->object-detection==0.1) (0.37.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.7.0->tf-models-official>=2.5.1->object-detection==0.1) (0.2.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.7.0->tf-models-official>=2.5.1->object-detection==0.1) (1.6.3)\n",
      "INFO: pip is looking at multiple versions of requests to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting requests<3.0.0,>=2.24.0\n",
      "  Downloading requests-2.25.1-py2.py3-none-any.whl (61 kB)\n",
      "     |████████████████████████████████| 61 kB 7.2 MB/s             \n",
      "\u001B[?25hINFO: pip is looking at multiple versions of pyyaml to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting pyyaml>=5.1\n",
      "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
      "     |████████████████████████████████| 636 kB 44.8 MB/s            \n",
      "\u001B[?25hINFO: pip is looking at multiple versions of pytz to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting pytz>=2017.2\n",
      "  Downloading pytz-2021.3-py2.py3-none-any.whl (503 kB)\n",
      "     |████████████████████████████████| 503 kB 43.3 MB/s            \n",
      "\u001B[?25hINFO: pip is looking at multiple versions of python-dateutil to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting python-dateutil>=2.7.3\n",
      "  Downloading python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
      "     |████████████████████████████████| 247 kB 46.4 MB/s            \n",
      "\u001B[?25hINFO: pip is looking at multiple versions of pyparsing to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting pyparsing>=2.4.0\n",
      "  Downloading pyparsing-3.0.6-py3-none-any.whl (97 kB)\n",
      "     |████████████████████████████████| 97 kB 6.3 MB/s             \n",
      "\u001B[?25hINFO: pip is looking at multiple versions of pymongo to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting pymongo<4.0.0,>=3.8.0\n",
      "  Downloading pymongo-3.12.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (508 kB)\n",
      "     |████████████████████████████████| 508 kB 44.5 MB/s            \n",
      "\u001B[?25hINFO: pip is looking at multiple versions of pydot to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting pydot<2,>=1.2.0\n",
      "  Downloading pydot-1.4.2-py2.py3-none-any.whl (21 kB)\n",
      "INFO: pip is looking at multiple versions of pyarrow to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting pyarrow<6.0.0,>=0.15.1\n",
      "  Downloading pyarrow-5.0.0-cp37-cp37m-manylinux2014_x86_64.whl (23.6 MB)\n",
      "     |████████████████████████████████| 23.6 MB 1.7 MB/s             \n",
      "\u001B[?25hINFO: pip is looking at multiple versions of py-cpuinfo to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting py-cpuinfo>=3.3.0\n",
      "  Downloading py-cpuinfo-7.0.0.tar.gz (95 kB)\n",
      "     |████████████████████████████████| 95 kB 4.6 MB/s             \n",
      "\u001B[?25h  Preparing metadata (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "INFO: pip is looking at multiple versions of psutil to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting psutil>=5.4.3\n",
      "  Downloading psutil-5.8.0-cp37-cp37m-manylinux2010_x86_64.whl (296 kB)\n",
      "     |████████████████████████████████| 296 kB 46.6 MB/s            \n",
      "\u001B[?25hINFO: pip is looking at multiple versions of protobuf to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting protobuf<4,>=3.12.2\n",
      "  Downloading protobuf-3.19.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "     |████████████████████████████████| 1.1 MB 19.9 MB/s            \n",
      "\u001B[?25hINFO: pip is looking at multiple versions of orjson to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting orjson<4.0\n",
      "  Downloading orjson-3.6.3-cp37-cp37m-manylinux_2_24_x86_64.whl (234 kB)\n",
      "     |████████████████████████████████| 234 kB 42.5 MB/s            \n",
      "\u001B[?25hINFO: pip is looking at multiple versions of opencv-python to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting opencv-python>=4.1.0.25\n",
      "  Downloading opencv_python-4.5.4.60-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (60.3 MB)\n",
      "     |████████████████████████████████| 60.3 MB 1.4 MB/s             \n",
      "\u001B[?25hINFO: pip is looking at multiple versions of oauth2client to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting oauth2client\n",
      "  Downloading oauth2client-4.1.3-py2.py3-none-any.whl (98 kB)\n",
      "     |████████████████████████████████| 98 kB 7.0 MB/s             \n",
      "\u001B[?25hINFO: pip is looking at multiple versions of numpy to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting numpy>=1.15.4\n",
      "  Downloading numpy-1.20.3-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.3 MB)\n",
      "     |████████████████████████████████| 15.3 MB 24.1 MB/s            \n",
      "\u001B[?25hINFO: pip is looking at multiple versions of kiwisolver to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting kiwisolver>=1.1.0\n",
      "  Downloading kiwisolver-1.3.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.1 MB)\n",
      "     |████████████████████████████████| 1.1 MB 36.0 MB/s            \n",
      "\u001B[?25hINFO: pip is looking at multiple versions of kaggle to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting kaggle>=1.3.9\n",
      "  Downloading kaggle-1.5.12.tar.gz (58 kB)\n",
      "     |████████████████████████████████| 58 kB 6.0 MB/s             \n",
      "\u001B[?25h  Preparing metadata (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "INFO: pip is looking at multiple versions of httplib2 to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting httplib2<0.20.0,>=0.8\n",
      "  Downloading httplib2-0.19.1-py3-none-any.whl (95 kB)\n",
      "     |████████████████████████████████| 95 kB 4.0 MB/s             \n",
      "\u001B[?25hINFO: pip is looking at multiple versions of hdfs to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting hdfs<3.0.0,>=2.1.0\n",
      "  Downloading hdfs-2.5.8.tar.gz (41 kB)\n",
      "     |████████████████████████████████| 41 kB 738 kB/s             \n",
      "\u001B[?25h  Preparing metadata (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "INFO: pip is looking at multiple versions of grpcio to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting grpcio<2,>=1.29.0\n",
      "  Downloading grpcio-1.42.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.0 MB)\n",
      "     |████████████████████████████████| 4.0 MB 30.9 MB/s            \n",
      "\u001B[?25hINFO: pip is looking at multiple versions of google-api-python-client to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting google-api-python-client>=1.6.7\n",
      "  Downloading google_api_python_client-2.32.0-py2.py3-none-any.whl (7.8 MB)\n",
      "     |████████████████████████████████| 7.8 MB 13.6 MB/s            \n",
      "\u001B[?25hINFO: pip is looking at multiple versions of future to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of fastavro to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting fastavro<2,>=0.21.4\n",
      "  Downloading fastavro-1.4.6-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n",
      "     |████████████████████████████████| 2.3 MB 34.7 MB/s            \n",
      "\u001B[?25hINFO: pip is looking at multiple versions of dill to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of cycler to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting cycler>=0.10.0\n",
      "  Downloading cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
      "INFO: pip is looking at multiple versions of crcmod to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting crcmod<2.0,>=1.7\n",
      "  Downloading crcmod-1.7.tar.gz (89 kB)\n",
      "     |████████████████████████████████| 89 kB 8.3 MB/s             \n",
      "\u001B[?25h  Preparing metadata (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "INFO: pip is looking at multiple versions of absl-py to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting absl-py>=0.2.2\n",
      "  Downloading absl_py-1.0.0-py3-none-any.whl (126 kB)\n",
      "     |████████████████████████████████| 126 kB 43.7 MB/s            \n",
      "\u001B[?25hINFO: pip is looking at multiple versions of tensorflow-io-gcs-filesystem to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting tensorflow-io-gcs-filesystem==0.22.0\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.22.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.1 MB)\n",
      "     |████████████████████████████████| 2.1 MB 25.6 MB/s            \n",
      "\u001B[?25hINFO: pip is looking at multiple versions of tensorflow-io to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting tensorflow_io\n",
      "  Downloading tensorflow_io-0.21.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (22.7 MB)\n",
      "     |████████████████████████████████| 22.7 MB 41.5 MB/s            \n",
      "\u001B[?25hINFO: pip is looking at multiple versions of pycocotools to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting pycocotools\n",
      "  Downloading pycocotools-2.0.3.tar.gz (106 kB)\n",
      "     |████████████████████████████████| 106 kB 45.6 MB/s            \n",
      "\u001B[?25h  Preparing metadata (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "INFO: pip is looking at multiple versions of pillow to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting pillow\n",
      "  Downloading Pillow-8.4.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "     |████████████████████████████████| 3.1 MB 19.0 MB/s            \n",
      "\u001B[?25hINFO: pip is looking at multiple versions of lxml to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting lxml\n",
      "  Downloading lxml-4.6.4-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (6.3 MB)\n",
      "     |████████████████████████████████| 6.3 MB 15.6 MB/s            \n",
      "\u001B[?25hINFO: pip is looking at multiple versions of six to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting six\n",
      "  Downloading six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
      "INFO: pip is looking at multiple versions of matplotlib to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.5.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (11.2 MB)\n",
      "     |████████████████████████████████| 11.2 MB 27.9 MB/s            \n",
      "\u001B[?25hINFO: pip is looking at multiple versions of lvis to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting lvis\n",
      "  Downloading lvis-0.5.2-py3-none-any.whl (14 kB)\n",
      "INFO: pip is looking at multiple versions of cython to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting Cython\n",
      "  Downloading Cython-0.29.24-cp37-cp37m-manylinux1_x86_64.whl (2.0 MB)\n",
      "     |████████████████████████████████| 2.0 MB 34.6 MB/s            \n",
      "\u001B[?25hINFO: pip is looking at multiple versions of contextlib2 to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting contextlib2\n",
      "  Downloading contextlib2-21.6.0-py2.py3-none-any.whl (13 kB)\n",
      "INFO: pip is looking at multiple versions of avro-python3 to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting avro-python3\n",
      "  Downloading avro-python3-1.9.1.tar.gz (36 kB)\n",
      "  Preparing metadata (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "INFO: pip is looking at multiple versions of apache-beam to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting apache-beam\n",
      "  Downloading apache_beam-2.33.0-cp37-cp37m-manylinux2010_x86_64.whl (9.8 MB)\n",
      "     |████████████████████████████████| 9.8 MB 18.6 MB/s            \n",
      "\u001B[?25hINFO: pip is looking at multiple versions of tf-slim to determine which version is compatible with other requirements. This could take a while.\n",
      "INFO: pip is looking at multiple versions of scipy to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting scipy\n",
      "  Downloading scipy-1.7.3-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (38.1 MB)\n",
      "     |████████████████████████████████| 38.1 MB 301 kB/s             \n",
      "\u001B[?25hINFO: pip is looking at multiple versions of pandas to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting pandas\n",
      "  Downloading pandas-1.3.4-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.3 MB)\n",
      "     |████████████████████████████████| 11.3 MB 35.1 MB/s            \n",
      "\u001B[?25hINFO: pip is looking at multiple versions of tf-models-official to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting tf-models-official>=2.5.1\n",
      "  Downloading tf_models_official-2.6.0-py2.py3-none-any.whl (1.8 MB)\n",
      "     |████████████████████████████████| 1.8 MB 35.7 MB/s            \n",
      "\u001B[?25hCollecting tensorflow>=2.5.0\n",
      "  Downloading tensorflow-2.6.2-cp37-cp37m-manylinux2010_x86_64.whl (458.3 MB)\n",
      "     |████████████████████████████████| 458.3 MB 12 kB/s              \n",
      "\u001B[?25hRequirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (1.1.0)\n",
      "Collecting wrapt~=1.12.1\n",
      "  Downloading wrapt-1.12.1.tar.gz (27 kB)\n",
      "  Preparing metadata (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "Collecting clang~=5.0\n",
      "  Downloading clang-5.0.tar.gz (30 kB)\n",
      "  Preparing metadata (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "Collecting tensorflow-estimator<2.7,>=2.6.0\n",
      "  Downloading tensorflow_estimator-2.6.0-py2.py3-none-any.whl (462 kB)\n",
      "     |████████████████████████████████| 462 kB 47.2 MB/s            \n",
      "\u001B[?25hRequirement already satisfied: h5py~=3.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (3.1.0)\n",
      "Requirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (0.4.0)\n",
      "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (1.1.2)\n",
      "Collecting tensorboard<2.7,>=2.6.0\n",
      "  Downloading tensorboard-2.6.0-py3-none-any.whl (5.6 MB)\n",
      "     |████████████████████████████████| 5.6 MB 28.5 MB/s            \n",
      "\u001B[?25hCollecting flatbuffers~=1.12.0\n",
      "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
      "Collecting typing-extensions~=3.7.4\n",
      "  Downloading typing_extensions-3.7.4.3-py3-none-any.whl (22 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem==0.21.0\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.21.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.1 MB)\n",
      "     |████████████████████████████████| 2.1 MB 35.9 MB/s            \n",
      "\u001B[?25hRequirement already satisfied: dm-tree~=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-model-optimization>=0.4.1->tf-models-official>=2.5.1->object-detection==0.1) (0.1.6)\n",
      "Collecting tensorflow-text>=2.5.0\n",
      "  Downloading tensorflow_text-2.7.0-cp37-cp37m-manylinux2010_x86_64.whl (4.9 MB)\n",
      "     |████████████████████████████████| 4.9 MB 33.5 MB/s            \n",
      "\u001B[?25h  Downloading tensorflow_text-2.6.0-cp37-cp37m-manylinux1_x86_64.whl (4.4 MB)\n",
      "     |████████████████████████████████| 4.4 MB 34.9 MB/s            \n",
      "\u001B[?25hCollecting portalocker\n",
      "  Downloading portalocker-2.3.2-py2.py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.7/dist-packages (from sacrebleu->tf-models-official>=2.5.1->object-detection==0.1) (0.8.9)\n",
      "Collecting colorama\n",
      "  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from sacrebleu->tf-models-official>=2.5.1->object-detection==0.1) (2019.12.20)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.7/dist-packages (from seqeval->tf-models-official>=2.5.1->object-detection==0.1) (1.0.1)\n",
      "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons->tf-models-official>=2.5.1->object-detection==0.1) (2.7.1)\n",
      "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tf-models-official>=2.5.1->object-detection==0.1) (5.4.0)\n",
      "Requirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tf-models-official>=2.5.1->object-detection==0.1) (21.2.0)\n",
      "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tf-models-official>=2.5.1->object-detection==0.1) (1.4.0)\n",
      "Requirement already satisfied: promise in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->tf-models-official>=2.5.1->object-detection==0.1) (2.3)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (1.53.0)\n",
      "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (21.3)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.16.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (4.2.4)\n",
      "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py~=3.1.0->tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (1.5.2)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official>=2.5.1->object-detection==0.1) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official>=2.5.1->object-detection==0.1) (3.0.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.7,>=2.6.0->tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (0.4.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.7,>=2.6.0->tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (0.6.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.7,>=2.6.0->tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (1.8.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.7,>=2.6.0->tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (1.0.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.7,>=2.6.0->tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (3.3.6)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources->tensorflow-datasets->tf-models-official>=2.5.1->object-detection==0.1) (3.6.0)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (1.3)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.7,>=2.6.0->tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.7,>=2.6.0->tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (4.8.2)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.7,>=2.6.0->tensorflow>=2.5.0->tf-models-official>=2.5.1->object-detection==0.1) (3.1.1)\n",
      "Building wheels for collected packages: object-detection, avro-python3, dill, future, py-cpuinfo, seqeval, clang, wrapt\n",
      "  Building wheel for object-detection (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "  Created wheel for object-detection: filename=object_detection-0.1-py3-none-any.whl size=21901864 sha256=d235170468c37a07c766e142c6d7ab6fee4188b474442857bbea4d586b75c99a\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-gt4zqx4v/wheels/fa/a4/d2/e9a5057e414fd46c8e543d2706cd836d64e1fcd9eccceb2329\n",
      "  Building wheel for avro-python3 (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "  Created wheel for avro-python3: filename=avro_python3-1.9.2.1-py3-none-any.whl size=43512 sha256=2a89e88f49326b1875d475d898f231bbc7503c3d6ed22b3763ea37528f8ff8fc\n",
      "  Stored in directory: /root/.cache/pip/wheels/bc/49/5f/fdb5b9d85055c478213e0158ac122b596816149a02d82e0ab1\n",
      "  Building wheel for dill (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "  Created wheel for dill: filename=dill-0.3.1.1-py3-none-any.whl size=78546 sha256=8dc9488ec9ceca17c6c9a05d6468acc66989b78bcbfc085af44d4ca37c23fb54\n",
      "  Stored in directory: /root/.cache/pip/wheels/a4/61/fd/c57e374e580aa78a45ed78d5859b3a44436af17e22ca53284f\n",
      "  Building wheel for future (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491070 sha256=97412069cb81f70bb2fbeee56a9b2411fa87fdbef7f5d305d283a35e1eb96d1c\n",
      "  Stored in directory: /root/.cache/pip/wheels/56/b0/fe/4410d17b32f1f0c3cf54cdfb2bc04d7b4b8f4ae377e2229ba0\n",
      "  Building wheel for py-cpuinfo (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "  Created wheel for py-cpuinfo: filename=py_cpuinfo-8.0.0-py3-none-any.whl size=22258 sha256=77aee1fd8ccdbb7612d274c4ee98b406f229753e22eff48eccf5a40ab05bc9db\n",
      "  Stored in directory: /root/.cache/pip/wheels/d2/f1/1f/041add21dc9c4220157f1bd2bd6afe1f1a49524c3396b94401\n",
      "  Building wheel for seqeval (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16181 sha256=75fc16a209af2fc31086ef69cd0175f2437425e630c1c89071af4bb913892d89\n",
      "  Stored in directory: /root/.cache/pip/wheels/05/96/ee/7cac4e74f3b19e3158dce26a20a1c86b3533c43ec72a549fd7\n",
      "  Building wheel for clang (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "  Created wheel for clang: filename=clang-5.0-py3-none-any.whl size=30692 sha256=da243bb5d6c47fd4563b8ac79a422d0f1469d2d5a6a6b3ebd04a361b58e27d7f\n",
      "  Stored in directory: /root/.cache/pip/wheels/98/91/04/971b4c587cf47ae952b108949b46926f426c02832d120a082a\n",
      "  Building wheel for wrapt (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "  Created wheel for wrapt: filename=wrapt-1.12.1-cp37-cp37m-linux_x86_64.whl size=68721 sha256=4a9f5a87159f3fd182445ef0baa5a77250085b20eadb7e341d7127706d8a057e\n",
      "  Stored in directory: /root/.cache/pip/wheels/62/76/4c/aa25851149f3f6d9785f6c869387ad82b3fd37582fa8147ac6\n",
      "Successfully built object-detection avro-python3 dill future py-cpuinfo seqeval clang wrapt\n",
      "Installing collected packages: typing-extensions, requests, wrapt, tensorflow-estimator, tensorboard, keras, flatbuffers, clang, tensorflow, portalocker, future, dill, colorama, tf-slim, tensorflow-text, tensorflow-model-optimization, tensorflow-io-gcs-filesystem, tensorflow-addons, seqeval, sentencepiece, sacrebleu, pyyaml, py-cpuinfo, orjson, opencv-python-headless, hdfs, fastavro, avro-python3, tf-models-official, tensorflow-io, lvis, apache-beam, object-detection\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing-extensions 3.10.0.2\n",
      "    Uninstalling typing-extensions-3.10.0.2:\n",
      "      Successfully uninstalled typing-extensions-3.10.0.2\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.23.0\n",
      "    Uninstalling requests-2.23.0:\n",
      "      Successfully uninstalled requests-2.23.0\n",
      "  Attempting uninstall: wrapt\n",
      "    Found existing installation: wrapt 1.13.3\n",
      "    Uninstalling wrapt-1.13.3:\n",
      "      Successfully uninstalled wrapt-1.13.3\n",
      "  Attempting uninstall: tensorflow-estimator\n",
      "    Found existing installation: tensorflow-estimator 2.7.0\n",
      "    Uninstalling tensorflow-estimator-2.7.0:\n",
      "      Successfully uninstalled tensorflow-estimator-2.7.0\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.7.0\n",
      "    Uninstalling tensorboard-2.7.0:\n",
      "      Successfully uninstalled tensorboard-2.7.0\n",
      "  Attempting uninstall: keras\n",
      "    Found existing installation: keras 2.7.0\n",
      "    Uninstalling keras-2.7.0:\n",
      "      Successfully uninstalled keras-2.7.0\n",
      "  Attempting uninstall: flatbuffers\n",
      "    Found existing installation: flatbuffers 2.0\n",
      "    Uninstalling flatbuffers-2.0:\n",
      "      Successfully uninstalled flatbuffers-2.0\n",
      "  Attempting uninstall: tensorflow\n",
      "    Found existing installation: tensorflow 2.7.0\n",
      "    Uninstalling tensorflow-2.7.0:\n",
      "      Successfully uninstalled tensorflow-2.7.0\n",
      "  Attempting uninstall: future\n",
      "    Found existing installation: future 0.16.0\n",
      "    Uninstalling future-0.16.0:\n",
      "      Successfully uninstalled future-0.16.0\n",
      "  Attempting uninstall: dill\n",
      "    Found existing installation: dill 0.3.4\n",
      "    Uninstalling dill-0.3.4:\n",
      "      Successfully uninstalled dill-0.3.4\n",
      "  Attempting uninstall: tensorflow-io-gcs-filesystem\n",
      "    Found existing installation: tensorflow-io-gcs-filesystem 0.22.0\n",
      "    Uninstalling tensorflow-io-gcs-filesystem-0.22.0:\n",
      "      Successfully uninstalled tensorflow-io-gcs-filesystem-0.22.0\n",
      "  Attempting uninstall: pyyaml\n",
      "    Found existing installation: PyYAML 3.13\n",
      "    Uninstalling PyYAML-3.13:\n",
      "      Successfully uninstalled PyYAML-3.13\n",
      "\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "multiprocess 0.70.12.2 requires dill>=0.3.4, but you have dill 0.3.1.1 which is incompatible.\n",
      "google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.26.0 which is incompatible.\n",
      "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001B[0m\n",
      "Successfully installed apache-beam-2.34.0 avro-python3-1.9.2.1 clang-5.0 colorama-0.4.4 dill-0.3.1.1 fastavro-1.4.7 flatbuffers-1.12 future-0.18.2 hdfs-2.6.0 keras-2.6.0 lvis-0.5.3 object-detection-0.1 opencv-python-headless-4.5.4.60 orjson-3.6.4 portalocker-2.3.2 py-cpuinfo-8.0.0 pyyaml-6.0 requests-2.26.0 sacrebleu-2.0.0 sentencepiece-0.1.96 seqeval-1.2.2 tensorboard-2.6.0 tensorflow-2.6.2 tensorflow-addons-0.15.0 tensorflow-estimator-2.6.0 tensorflow-io-0.21.0 tensorflow-io-gcs-filesystem-0.21.0 tensorflow-model-optimization-0.7.0 tensorflow-text-2.6.0 tf-models-official-2.6.0 tf-slim-1.1.0 typing-extensions-3.7.4.3 wrapt-1.12.1\n",
      "\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "X7sdaToKifaS"
   },
   "source": [
    "'''\n",
    "  FUNCTION TO CONVERT THE IMAGES + ANNOTATIONS\n",
    "  INTO TF RECORDS USING THE OFFICIAL TUTORIAL\n",
    "  https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/using_your_own_dataset.md\n",
    "'''\n",
    "\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import absolute_import\n",
    "\n",
    "import os\n",
    "import io\n",
    "import pandas as pd\n",
    "import tensorflow.compat.v1 as tf\n",
    "\n",
    "from PIL import Image\n",
    "from object_detection.utils import dataset_util\n",
    "from collections import namedtuple, OrderedDict\n",
    "\n",
    "\"\"\"\n",
    "  FUNCTION MAPPING NAME TO NUMBER\n",
    "  NECESSARY TO CREATE THE TF RECORDS FOR THIS SPECIFIC PROJECT\n",
    "\"\"\"\n",
    "def class_text_to_int(row_label):\n",
    "    if row_label == 'Nike':\n",
    "        return 1\n",
    "    elif row_label == 'Adidas':\n",
    "        return 2\n",
    "    elif row_label == 'Starbucks':\n",
    "        return 3\n",
    "    elif row_label == 'Apple Inc.':\n",
    "        return 4\n",
    "    elif row_label == 'NFL':\n",
    "        return 5\n",
    "    elif row_label == 'Mercedes-Benz':\n",
    "        return 6\n",
    "    elif row_label == 'Under Armour':\n",
    "        return 7\n",
    "    elif row_label == 'Coca-Cola':\n",
    "        return 8\n",
    "    elif row_label == 'Hard Rock Cafe':\n",
    "        return 9\n",
    "    elif row_label == 'Puma':\n",
    "        return 10\n",
    "    elif row_label == 'The North Face':\n",
    "        return 11\n",
    "    elif row_label == 'Toyota':\n",
    "        return 12\n",
    "    elif row_label == 'Chanel':\n",
    "        return 13\n",
    "    elif row_label == 'Pepsi':\n",
    "        return 14\n",
    "    else:\n",
    "        None\n",
    "\n",
    "\"\"\"\n",
    "  FUNCTION TO GROUP ANNOTATIONS PER IMAGE,\n",
    "  IN CASE SOME IMAGES HAS MORE THAN ONE ANNOTATION\n",
    "  NEED THE DATAFRAME WITH ANNOTATIONS AND THE NAME OF THE COLUMN\n",
    "  WHERE THERE IS THE PHOTO FILE NAME\n",
    "  DF = DATAFRAME WITH ANNOTATIONS\n",
    "  GROUP = NAME OF THE COLUMN CONTAINING THE IMAGE NAME\n",
    "\"\"\"\n",
    "def split(df, group):\n",
    "    data = namedtuple('data', ['filename', 'object'])\n",
    "    gb = df.groupby(group)\n",
    "    return [data(filename, gb.get_group(x)) for filename, x in zip(gb.groups.keys(), gb.groups)]\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "  FUNCTION TO CREATE THE TF RECORDS PER IMAGE\n",
    "  GROUP = SINGLE IMAGE GROUPPED\n",
    "  PATH = PATH TO IMAGE DIRECTORY\n",
    "\"\"\"\n",
    "def create_tf_example(group, path):\n",
    "    with tf.gfile.GFile(os.path.join(path, '{}'.format(group.filename)), 'rb') as fid:\n",
    "        encoded_jpg = fid.read() # READ THE IMAGE\n",
    "    encoded_jpg_io = io.BytesIO(encoded_jpg) # CONVERT TO BYTES\n",
    "    image = Image.open(encoded_jpg_io) # OPEN IMAGE WITH IMAGE LIBRARY\n",
    "    width, height = image.size # GET THE WIDHT AND HEIGHT\n",
    "\n",
    "    filename = group.filename.encode('utf8')\n",
    "    image_format = b'jpg'\n",
    "    xmins = []\n",
    "    xmaxs = []\n",
    "    ymins = []\n",
    "    ymaxs = []\n",
    "    classes_text = []\n",
    "    classes = []\n",
    "\n",
    "    for index, row in group.object.iterrows():\n",
    "        xmins.append(row['xmin'] / width)\n",
    "        xmaxs.append(row['xmax'] / width)\n",
    "        ymins.append(row['ymin'] / height)\n",
    "        ymaxs.append(row['ymax'] / height)\n",
    "        classes_text.append(row['class'].encode('utf8'))\n",
    "        classes.append(class_text_to_int(row['class']))\n",
    "\n",
    "    tf_example = tf.train.Example(features=tf.train.Features(feature={\n",
    "        'image/height': dataset_util.int64_feature(height),\n",
    "        'image/width': dataset_util.int64_feature(width),\n",
    "        'image/filename': dataset_util.bytes_feature(filename),\n",
    "        'image/source_id': dataset_util.bytes_feature(filename),\n",
    "        'image/encoded': dataset_util.bytes_feature(encoded_jpg),\n",
    "        'image/format': dataset_util.bytes_feature(image_format),\n",
    "        'image/object/bbox/xmin': dataset_util.float_list_feature(xmins),\n",
    "        'image/object/bbox/xmax': dataset_util.float_list_feature(xmaxs),\n",
    "        'image/object/bbox/ymin': dataset_util.float_list_feature(ymins),\n",
    "        'image/object/bbox/ymax': dataset_util.float_list_feature(ymaxs),\n",
    "        'image/object/class/text': dataset_util.bytes_list_feature(classes_text),\n",
    "        'image/object/class/label': dataset_util.int64_list_feature(classes),\n",
    "    }))\n",
    "    return tf_example\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "  FUCTION THAT USES THE 2 PREVIOUS FUNCTIONS\n",
    "  OUTPUT_PATH = PATH TO SAVE THE TF RECORDS\n",
    "  IMG_DIR = PATH TO IMAGE DIRECTORY\n",
    "  CSV_INPUT = ANNOTATIONS OF ALL IMAGES FROM IMG_DIR\n",
    "\"\"\"\n",
    "def generate_tfrecord(output_path,img_dir,csv_input):\n",
    "    writer = tf.io.TFRecordWriter(output_path)\n",
    "    path = os.path.join(img_dir)\n",
    "    examples = pd.read_csv(csv_input)\n",
    "    grouped = split(examples, 'photo_filename')\n",
    "    for group in grouped: #for each image in groupped images\n",
    "        tf_example = create_tf_example(group, path)\n",
    "        writer.write(tf_example.SerializeToString())\n",
    "\n",
    "    writer.close()\n",
    "    print('Successfully created the TFRecords: {}'.format(output_path))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "r6a3nHldQnam",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "aaa8a46a-8c9c-417c-96a6-c81917d0af01"
   },
   "source": [
    "# CREATING TF RECORDS FOR THE TRAINING IMAGES\n",
    "\n",
    "output_path = '/content/data/train.tfrecord'\n",
    "img_dir = '/content/data/train'\n",
    "csv_input= '/content/data/train.csv'\n",
    "generate_tfrecord(output_path,img_dir,csv_input) "
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Successfully created the TFRecords: /content/data/train.tfrecord\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "25Ytqj_LQn8l",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "ee671ef2-6c78-410b-826f-37807b94fae1"
   },
   "source": [
    "# CREATING TF RECORDS FOR THE VALIDATION IMAGES\n",
    "\n",
    "output_path = '/content/data/valid.tfrecord'\n",
    "img_dir = '/content/data/valid'\n",
    "csv_input= '/content/data/valid.csv'\n",
    "generate_tfrecord(output_path,img_dir,csv_input) "
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Successfully created the TFRecords: /content/data/valid.tfrecord\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hR_tdyZ2nRCo"
   },
   "source": [
    "The following files were coppied to our shared drive folder:    \n",
    "- /content/data/test\n",
    "- /content/data/test.csv\n",
    "- /content/data/train.csv\n",
    "- /content/data/valid.csv\n",
    "- /content/data/train.tfrecord\n",
    "- /content/data/valid.tfrecord\n",
    "\n",
    "The others folders/files are not used neither for training nor for testing the model."
   ]
  }
 ]
}